{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does nn.Conv2d init work well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_02 import *\n",
    "\n",
    "def get_data():\n",
    "    path = datasets.download_data(MNIST_URL, ext='.gz')\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "    return map(tensor, (x_train,y_train,x_valid,y_valid))\n",
    "\n",
    "def normalize(x, m, s): return (x-m)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mSignature:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m <no docstring>\n\u001b[0;31mSource:\u001b[0m   \n    \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0mfan_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_fan_in_and_fan_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfan_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m            \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mFile:\u001b[0m      /usr/local/lib/python3.7/site-packages/torch/nn/modules/conv.py\n\u001b[0;31mType:\u001b[0m      function\n",
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "torch.nn.modules.conv._ConvNd.reset_parameters??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_valid,y_valid = get_data()\n",
    "train_mean,train_std = x_train.mean(),x_train.std()\n",
    "x_train = normalize(x_train, train_mean, train_std)\n",
    "x_valid = normalize(x_valid, train_mean, train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(torch.Size([50000, 1, 28, 28]), torch.Size([10000, 1, 28, 28]))"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Resizing\n",
    "x_train = x_train.view(-1,1,28,28)\n",
    "x_valid = x_valid.view(-1,1,28,28)\n",
    "x_train.shape,x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(50000, tensor(10))"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "n,*_ = x_train.shape\n",
    "c = y_train.max()+1\n",
    "nh = 32\n",
    "n,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = nn.Conv2d(1, nh, 5) # 1 input/channel, 32 outputs, 5 x 5 kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_valid[:100] # grabbing first 100 for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([100, 1, 28, 28])"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(x): return x.mean(),x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([32, 1, 5, 5])"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "l1.weight.shape # 32 output filters, 1 input filter/channel, 5 x 5 size of our tensor. (5x5 - see CNN eg from part-1. Go through Part-1 again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((tensor(-0.0063, grad_fn=<MeanBackward0>),\n  tensor(0.1153, grad_fn=<StdBackward0>)),\n (tensor(-0.0496, grad_fn=<MeanBackward0>),\n  tensor(0.1127, grad_fn=<StdBackward0>)))"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "stats(l1.weight),stats(l1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = l1(x) #O/P of first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor(-0.0655, grad_fn=<MeanBackward0>),\n tensor(0.6656, grad_fn=<StdBackward0>))"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "stats(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor(-0.0711, grad_fn=<MeanBackward0>),\n tensor(1.0093, grad_fn=<StdBackward0>))"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "init.kaiming_normal_(l1.weight, a=1.) # For ReLU, we have no Leak. What is Leak? See OneNote. So in order to adjust for a non-relu Kaiming Init behavior, we make the leak(a) as = 1.\n",
    "stats(l1(x))\n",
    "# See the the maen and std dev are way better than the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x,a=0): return F.leaky_relu(l1(x),a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor(0.4412, grad_fn=<MeanBackward0>),\n tensor(0.9136, grad_fn=<StdBackward0>))"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# we then use the Kaiming Init for ReLU (a=0) to initialize our weights\n",
    "init.kaiming_normal_(l1.weight, a=0)\n",
    "stats(f1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>We will write out own Kaming Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor(0.2191, grad_fn=<MeanBackward0>),\n tensor(0.4092, grad_fn=<StdBackward0>))"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "l1 = nn.Conv2d(1, nh, 5)\n",
    "stats(f1(x)) #remember the defaul PyTorch lib doesn't handle the ReLU case explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([32, 1, 5, 5])"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "l1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([5, 5])"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "l1.weight[0,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "25"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "# receptive field size\n",
    "rec_fs = l1.weight[0,0].numel() # just taking l1.weight[0.0] to get the 5 x 5 dim\n",
    "rec_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([32, 1, 5, 5])"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "l1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(32, 1)"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "nf,ni,*_ = l1.weight.shape # number_of_filters, number_of_inputs\n",
    "nf,ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(25, 800)"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "fan_in  = ni*rec_fs #number_of_inputs * receptive_fs\n",
    "fan_out = nf*rec_fs\n",
    "fan_in,fan_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain(a): return math.sqrt(2.0 / (1 + a**2)) # Kaiming init gain formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(1.0,\n 1.4142135623730951,\n 1.4141428569978354,\n 1.4071950894605838,\n 0.5773502691896257)"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "# The below will explain the leaky gain phenomenon where the value of a ranges from 1 to sqrt(5) which is as in the PyTorch Docs (cell 3 from the top): init.kaiming_uniform_(self.weight, a=math.sqrt(5)). One thing to note is that this accounts for a uniform kaiming and not a normal kaiming but still doesn't explain the .57 value (not sure why)\n",
    "gain(1),gain(0),gain(0.01),gain(0.1),gain(math.sqrt(5.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(0.5751)"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "torch.zeros(10000).uniform_(-1,1).std() #Jeremy just took a random sample of 10k random numbers to see what the STD looks like for a UNIFORM dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.5773502691896258"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "1/math.sqrt(3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jeremy's Kaiming\n",
    "def kaiming2(x,a, use_fan_out=False):\n",
    "    nf,ni,*_ = x.shape\n",
    "    rec_fs = x[0,0].shape.numel()\n",
    "    fan = nf*rec_fs if use_fan_out else ni*rec_fs\n",
    "    std = gain(a) / math.sqrt(fan)\n",
    "    bound = math.sqrt(3.) * std\n",
    "    x.data.uniform_(-bound,bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor(0.4921, grad_fn=<MeanBackward0>),\n tensor(0.9033, grad_fn=<StdBackward0>))"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "kaiming2(l1.weight, a=0)\n",
    "stats(f1(x)) # these values are so much closer to normality compared to the PyTorch native src "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor(0.2215, grad_fn=<MeanBackward0>),\n tensor(0.3884, grad_fn=<StdBackward0>))"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "kaiming2(l1.weight, a=math.sqrt(5.))\n",
    "stats(f1(x)) # Using the sqrt(5) to check if we get same values as the PyTorch library and we do. We did this in cells above using this:\n",
    "# l1 = nn.Conv2d(1, nh, 5)\n",
    "# stats(f1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a sample 4 layer CNN using PyTorch defaults and run to see how the output stats look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self,x): return x.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sequential(\n",
    "    nn.Conv2d(1,8, 5,stride=2,padding=2), nn.ReLU(),\n",
    "    nn.Conv2d(8,16,3,stride=2,padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(16,32,3,stride=2,padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(32,1,3,stride=2,padding=1),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Flatten(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_valid[:100].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor(-0.0442, grad_fn=<MeanBackward0>),\n tensor(0.0085, grad_fn=<StdBackward0>))"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "t = m(x)\n",
    "stats(t) #not close to NORMAL at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = mse(t,y)\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor(0.0236), tensor(0.0326))"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "stats(m[0].weight.grad) # checking the stats n the weight and if you look at the variance, it is very low "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mSignature:\u001b[0m\n\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fan_in'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mnonlinearity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'leaky_relu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mSource:\u001b[0m   \n\u001b[0;32mdef\u001b[0m \u001b[0mkaiming_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fan_in'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonlinearity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'leaky_relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;34mr\"\"\"Fills the input `Tensor` with values according to the method\u001b[0m\n\u001b[0;34m    described in `Delving deep into rectifiers: Surpassing human-level\u001b[0m\n\u001b[0;34m    performance on ImageNet classification` - He, K. et al. (2015), using a\u001b[0m\n\u001b[0;34m    uniform distribution. The resulting tensor will have values sampled from\u001b[0m\n\u001b[0;34m    :math:`\\mathcal{U}(-\\text{bound}, \\text{bound})` where\u001b[0m\n\u001b[0;34m\u001b[0m\n\u001b[0;34m    .. math::\u001b[0m\n\u001b[0;34m        \\text{bound} = \\text{gain} \\times \\sqrt{\\frac{3}{\\text{fan\\_mode}}}\u001b[0m\n\u001b[0;34m\u001b[0m\n\u001b[0;34m    Also known as He initialization.\u001b[0m\n\u001b[0;34m\u001b[0m\n\u001b[0;34m    Args:\u001b[0m\n\u001b[0;34m        tensor: an n-dimensional `torch.Tensor`\u001b[0m\n\u001b[0;34m        a: the negative slope of the rectifier used after this layer (only \u001b[0m\n\u001b[0;34m        used with ``'leaky_relu'``)\u001b[0m\n\u001b[0;34m        mode: either ``'fan_in'`` (default) or ``'fan_out'``. Choosing ``'fan_in'``\u001b[0m\n\u001b[0;34m            preserves the magnitude of the variance of the weights in the\u001b[0m\n\u001b[0;34m            forward pass. Choosing ``'fan_out'`` preserves the magnitudes in the\u001b[0m\n\u001b[0;34m            backwards pass.\u001b[0m\n\u001b[0;34m        nonlinearity: the non-linear function (`nn.functional` name),\u001b[0m\n\u001b[0;34m            recommended to use only with ``'relu'`` or ``'leaky_relu'`` (default).\u001b[0m\n\u001b[0;34m\u001b[0m\n\u001b[0;34m    Examples:\u001b[0m\n\u001b[0;34m        >>> w = torch.empty(3, 5)\u001b[0m\n\u001b[0;34m        >>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')\u001b[0m\n\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mfan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_calculate_correct_fan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mgain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnonlinearity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgain\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstd\u001b[0m  \u001b[0;31m# Calculate uniform bounds from standard deviation\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mFile:\u001b[0m      /usr/local/lib/python3.7/site-packages/torch/nn/init.py\n\u001b[0;31mType:\u001b[0m      function\n",
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "init.kaiming_uniform_??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in m:\n",
    "    if isinstance(l,nn.Conv2d):\n",
    "        init.kaiming_uniform_(l.weight)\n",
    "        l.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor(-0.4908, grad_fn=<MeanBackward0>),\n tensor(0.2116, grad_fn=<StdBackward0>))"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "t = m(x)\n",
    "stats(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor(0.0452), tensor(0.4491))"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "l = mse(t,y)\n",
    "l.backward()\n",
    "stats(m[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./notebook2script.py 02a_why_sqrt5.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}